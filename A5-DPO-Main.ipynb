{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Direct Preference Optimization: Your Language Model is Secretly a Reward Model (DPO)](https://arxiv.org/pdf/2305.18290.pdf)\n",
    "\n",
    "### Reference Code \n",
    "- https://huggingface.co/docs/trl/main/en/dpo_trainer\n",
    "- https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the final dataset object should contain these 3 entries if you use the default DPODataCollatorWithPadding data collator. \n",
    "\n",
    "The entries should be named:\n",
    "- prompt\n",
    "- chosen\n",
    "- rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "# os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "#device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device =torch.device(\"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_dataset_dict = {\n",
    "    \"prompt\": [\n",
    "        \"hello\",\n",
    "        \"how are you\",\n",
    "        \"What is your name?\",\n",
    "        \"What is your name?\",\n",
    "        \"Which is the best programming language?\",\n",
    "        \"Which is the best programming language?\",\n",
    "        \"Which is the best programming language?\",\n",
    "    ],\n",
    "    \"chosen\": [\n",
    "        \"hi nice to meet you\",\n",
    "        \"I am fine\",\n",
    "        \"My name is Mary\",\n",
    "        \"My name is Mary\",\n",
    "        \"Python\",\n",
    "        \"Python\",\n",
    "        \"Java\",\n",
    "    ],\n",
    "    \"rejected\": [\n",
    "        \"leave me alone\",\n",
    "        \"I am not fine\",\n",
    "        \"Whats it to you?\",\n",
    "        \"I dont have a name\",\n",
    "        \"Javascript\",\n",
    "        \"C++\",\n",
    "        \"C++\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    HfArgumentParser, \n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from typing import Dict, Optional\n",
    "from trl import DPOTrainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load a pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125171/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"gpt2\"\n",
    "ignore_bias_buffers = False\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "# model.to(device)\n",
    "# model.gradient_checkpointing_enable()\n",
    "\n",
    "if ignore_bias_buffers:\n",
    "    # torch distributed hack\n",
    "    model._ddp_params_and_buffers_to_ignore = [\n",
    "        name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool\n",
    "    ]\n",
    "\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "# model_ref.to(device)\n",
    "# model_ref.gradient_checkpointing_enable()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DPO trainer expects a model of AutoModelForCausalLM, compared to PPO that expects AutoModelForCausalLMWithValueHead for the value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **Task 1. Finding a Suitable Dataset** (0.5 point)\n",
    "\n",
    "1) Select a publicly available dataset for preference optimization tasks, such as human preference rankings or reinforcement learning from human feedback (RLHF) datasets.\n",
    "   \n",
    "2) Ensure that the dataset is properly preprocessed and suitable for training a preference-based model. \n",
    "   \n",
    "3) Document the dataset source and preprocessing steps.\n",
    "   \n",
    "**NOTE**: You can use datasets from Hugging Face Datasets Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "**Selected Dataset**: https://huggingface.co/datasets/Dahoas/rm-hh-rlhf\n",
    "\n",
    "The dataset holds human preference rankings which makes it suitable to learn reinforcement learning from human feedback (RLHF).\n",
    "\n",
    "**Preprocessing & Suitability**:\n",
    "-  The dataset already contains \"prompt\", \"chosen\" and \"rejected\" labels that structure it for preference models.\n",
    "\n",
    "**Basic preprocessing steps**:\n",
    "- The text input requires tokenization through a suitable tokenizer. Here I a have used GPT2.\n",
    "- Text entries need transforming into numeric representation (such as embeddings).\n",
    "- The training must follow batched input arrangements for maximum processing speed.\n",
    "\n",
    "**Dataset Source & Preprocessing Documentation**:\n",
    "- The dataset exists in the Hugging Face Datasets Hub under (Dahoas/rm-hh-rlhf).\n",
    "\n",
    "**Preprocessing Steps**:\n",
    "- Load dataset using datasets.load_dataset(\"Dahoas/rm-hh-rlhf\")\n",
    "- Tokenize input text\n",
    "- The data must be formatted using methods for preference learning that include pairwise ranking loss.\n",
    "- Split into training/validation/test sets\n",
    "\n",
    "The dataset provides excellent conditions for training a reward model that optimizes preferences in RLHF-based applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Anthropic Helpful-Harmless dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dahoas_hh(split: str, sanity_check: bool = False, cache_dir: str = None) -> Dataset:\n",
    "    \"\"\"\n",
    "    Load the Dahoas RM HH dataset and return it in the correct format.\n",
    "\n",
    "    The dataset is converted to a dictionary with the following structure:\n",
    "    {\n",
    "        'prompt': List[str],\n",
    "        'chosen': List[str],\n",
    "        'rejected': List[str],\n",
    "    }\n",
    "\n",
    "    Unlike the Anthropic dataset, this dataset already provides the 'prompt' field.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load dataset from Hugging Face\n",
    "    dataset = load_dataset(\"Dahoas/rm-hh-rlhf\", split=split, cache_dir=cache_dir)\n",
    "    \n",
    "    # Apply sanity check (limit dataset to 1000 samples)\n",
    "    if sanity_check:\n",
    "        dataset = dataset.select(range(min(len(dataset), 1000)))\n",
    "\n",
    "    def format_sample(sample: Dict[str, str]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Ensure the dataset is correctly formatted.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"prompt\": sample[\"prompt\"],  # Use provided prompt directly\n",
    "            \"chosen\": sample[\"chosen\"],  # Preferred response\n",
    "            \"rejected\": sample[\"rejected\"],  # Rejected response\n",
    "        }\n",
    "\n",
    "    return dataset.map(format_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sanity_check = True\n",
    "train_dataset = get_dahoas_hh(\"train\", sanity_check=sanity_check)\n",
    "eval_dataset = get_dahoas_hh(\"test\", sanity_check=sanity_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **Task 2. Training a Model with DPOTrainer**\n",
    "\n",
    "1) Implement the Direct Preference Optimization (DPO) training method with **DPOTrainer** Function using a pre-trained transformer model (such as GPT, or T5) on the Hugging Face and fine-tune it using the selected dataset. (1 point)\n",
    "\n",
    "2) Experiment with hyperparameters and report training performance. (1 point)\n",
    "\n",
    "**HINT**: Refer to the Hugging Face documentation for **DPOTrainer** implementation.\n",
    "\n",
    "**Note**: You do not need to train large model sizes like 1B-7B if your GPU is not capable. This assignment focuses on how to use pre-trained models with Hugging Face."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. initialize training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 1\n",
    "max_length= 512 \n",
    "max_prompt_length = 128 \n",
    "max_target_length =128 \n",
    "label_pad_token_id = 100\n",
    "max_steps = 1000\n",
    "# instrumentation\n",
    "sanity_check = True\n",
    "report_to = None\n",
    "gradient_checkpointing = None\n",
    "beta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.gradient_checkpointing = True\n",
    "# model_ref.config.gradient_checkpointing = True\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    max_steps=max_steps,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    logging_steps=5,  # match results in blog post\n",
    "    eval_steps=500,\n",
    "    output_dir=\"./test\",\n",
    "    optim=\"rmsprop\",\n",
    "    warmup_steps=150,\n",
    "    report_to=report_to,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    # TODO: uncomment that on the next transformers release\n",
    "    # gradient_checkpointing_kwargs=gradient_checkpointing_kwargs,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. initialize the DPO trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e0db7fd2b9461b90e236d20a344ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1209 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb79d987c6a49f9a7e9bfa999678059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref,\n",
    "    args=training_args,\n",
    "    beta=beta,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    max_target_length=max_target_length,\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    generate_during_eval=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mst125171\u001b[0m (\u001b[33mbinit-ait\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter-st125171/A5-DPO/wandb/run-20250227_172155-rcspvskc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/binit-ait/huggingface/runs/rcspvskc' target=\"_blank\">./test</a></strong> to <a href='https://wandb.ai/binit-ait/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/binit-ait/huggingface' target=\"_blank\">https://wandb.ai/binit-ait/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/binit-ait/huggingface/runs/rcspvskc' target=\"_blank\">https://wandb.ai/binit-ait/huggingface/runs/rcspvskc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 05:05, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.657900</td>\n",
       "      <td>6.222948</td>\n",
       "      <td>-29.091265</td>\n",
       "      <td>-28.120049</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>-0.971216</td>\n",
       "      <td>-500.237701</td>\n",
       "      <td>-526.666809</td>\n",
       "      <td>-20.210629</td>\n",
       "      <td>-19.036877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>8.201928</td>\n",
       "      <td>-36.209774</td>\n",
       "      <td>-34.843243</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>-1.366528</td>\n",
       "      <td>-567.469604</td>\n",
       "      <td>-597.851868</td>\n",
       "      <td>-35.477596</td>\n",
       "      <td>-32.511292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=3.6265436560861417, metrics={'train_runtime': 308.0019, 'train_samples_per_second': 12.987, 'train_steps_per_second': 3.247, 'total_flos': 0.0, 'train_loss': 3.6265436560861417, 'epoch': 4.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 Answer**:\n",
    "\n",
    "**Observations & Insights**\n",
    "- The model showed successful training through the large reduction of training loss which changed from 4.6579 to 0.0320.\n",
    "- The validation loss experienced a considerable rise from 6.2229 to 8.2019 showing evidence that could be explained by both model overfitting and instability in optimization.\n",
    "- The model maintained similar reward accuracy levels because it decreased slightly from 0.460 to 0.452 which indicated limited generalization progress.\n",
    "- The scoring instability for both chosen and rejected responses emerges from the negative drift in logits values while showing an expanded preference gap between them.\n",
    "\n",
    "The model's performance improved significantly after the introduction of DPO, indicating that it can handle and optimize preferences in RLHF-based applications more effectively.\n",
    "\n",
    "\n",
    "**Hyperparameters Used**:  \n",
    "Learning Rate: 1e-3  \n",
    "Batch Size: 4  \n",
    "Gradient Accumulation Steps: 1  \n",
    "Max Steps: 1000  \n",
    "Max Sequence Length: 512 (Prompt: 128, Target: 128)  \n",
    "Beta: 0.1  \n",
    "\n",
    "\n",
    "**Hyperparameters & Experiments / Possible Improvements**:\n",
    "- Experimenting with different learning rates (1e-3, 1e-4, 1e-5) and batch sizes (4, 8, 16) may improve model's performance significantly with larger batch sizes and lower learning rates.\n",
    "\n",
    "**Charts from Experimenting**:\n",
    "| Step | Training Loss | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |\n",
    "|------|--------------|----------------|----------------|------------------|---------------------|-----------------|----------------|--------------|----------------|--------------|\n",
    "| 500  | 4.657900    | 6.222948       | -29.091265     | -28.120049       | 0.460000            | -0.971216       | -500.237701    | -526.666809  | -20.210629     | -19.036877  |\n",
    "| 1000 | 0.032000    | 8.201928       | -36.209774     | -34.843243       | 0.452000            | -1.366528       | -567.469604    | -597.851868  | -35.477596     | -32.511292  |\n",
    "\n",
    "\n",
    "<h5>Training</h5>\n",
    "<p align=\"left\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb1.png\" width=\"30%\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb2.png\" width=\"30%\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb3.png\" width=\"30%\">\n",
    "</p>\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb4.png\" width=\"30%\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb5.png\" width=\"30%\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb6.png\" width=\"30%\">\n",
    "</p>\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb7.png\" width=\"30%\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb8.png\" width=\"30%\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb9.png\" width=\"30%\">\n",
    "</p>\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb10.png\" width=\"30%\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb11.png\" width=\"30%\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb12.png\" width=\"30%\">\n",
    "</p>\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb13.png\" width=\"30%\">\n",
    "</p>\n",
    "\n",
    "<h5>Eval</h5>\n",
    "<p align=\"left\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb14.png\" width=\"30%\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb15.png\" width=\"30%\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb16.png\" width=\"30%\">\n",
    "</p>\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb17.png\" width=\"30%\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb18.png\" width=\"30%\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb19.png\" width=\"30%\">\n",
    "</p>\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb20.png\" width=\"30%\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb21.png\" width=\"30%\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb22.png\" width=\"30%\">\n",
    "</p>\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb23.png\" width=\"30%\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb24.png\" width=\"30%\">\n",
    "  <img src=\"./screenshots/charts/Screenshot_wandb25.png\" width=\"30%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./dpo_model/tokenizer_config.json',\n",
       " './dpo_model/special_tokens_map.json',\n",
       " './dpo_model/vocab.json',\n",
       " './dpo_model/merges.txt',\n",
       " './dpo_model/added_tokens.json',\n",
       " './dpo_model/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save_pretrained(\"./dpo_model\")\n",
    "tokenizer.save_pretrained(\"./dpo_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **Task 3. Pushing the Model to Hugging Face Hub** (0.5 point)\n",
    "1) Save the trained model.\n",
    "\n",
    "2) Upload the model to the Hugging Face Model Hub.\n",
    "\n",
    "3) Provide a link to your uploaded model in your documentation.\n",
    "\n",
    "**NOTE**: Make sure your repository is public and also the README.md should also contain the link to your publicly available trained model on Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "1. The trained model has been saved in the \"./dpo_model\" directory.\n",
    "2. The code for this is available in the HF_push.ipynb file.\n",
    "3. Link to uploaded model in hugging Face: https://huggingface.co/sachinmalego/DPO_Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **Task 4. Web Application Development** (1 point)\n",
    "1) Develop a simple web application that demonstrates your trained model's capabilities. \n",
    "   \n",
    "2) The app should allow users to input text and receive response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "##### **Web application can be accessed locally**:  \n",
    "To deploy application first download repo from github (https://github.com/sachinmalego/NLP-A5-Optimization-Human-Preference.git).   \n",
    "\n",
    "Open in VSCode and open terminal.  \n",
    "In the terminal type \"python3 app.py\". My local deployment address was \"http://127.0.0.1:5000/\" however your's might be different.  \n",
    "Go to browser and enter your local deployment server address to test the application. \n",
    "\n",
    "Video of Working application:  \n",
    "Link to video: https://drive.google.com/file/d/16MoIoCSuI5tKw_OS4qSWsw2kKCJMrbMP/view?usp=sharing\n",
    "\n",
    "\n",
    "![Fig 1. Video](./screenshots/A5-DPO.gif)\n",
    "\n",
    "Screen shots of the working application is attached here with: \n",
    "\n",
    "![Fig 2. Screenshot1](./screenshots/Screenshot1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
